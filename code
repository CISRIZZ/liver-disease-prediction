```
import numpy as np   #Importing necessary libraries to begin the code with seaborn for connecting pandas and the dataset for plt and warnings are ignored hor a hassle-free coding session!
```
```
import pandas as pd
```
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


#We imported os to lead pandas access the file
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
patients=pd.read_csv('/kaggle/input/indian-liver-patient-records/indian_liver_patient.csv')

patients.head() #Reviewing our data
patients.shape #Checking our rows and columns
patients['Gender']=patients['Gender'].apply(lambda x:1 if x=='Male' else 0) #1 is male =, 0 is female
patients.head()
patients['Gender'].value_counts().plot.bar(color='peachpuff') #Plotting how many men vs. women have the disease
patients['Dataset'].value_counts().plot.bar(color='blue') #Checking how many in general have the disease vs those who dont
patients.isnull().sum() #Searching for null values == 4 found
patients['Albumin_and_Globulin_Ratio'].mean() #Filling in null value via .mean(), .fillna() function
patients=patients.fillna(0.94)
patients=patients.fillna(0.94)
sns.set_style('darkgrid')
plt.figure(figsize=(25,10))
patients['Age'].value_counts().plot.bar(color='black')
plt.figure(figsize=(8,6)) #Comparing protein take between both genders
patients.groupby('Gender').sum()["Total_Protiens"].plot.bar(color='orange')
plt.figure(figsize=(8,6))  #Comparing albumin between women and men
patients.groupby('Gender').sum()['Albumin'].plot.bar(color='skyblue')
plt.figure(figsize=(8,6)) #Comparing Total Bilirubin between men and women
patients.groupby('Gender').sum()['Total_Bilirubin'].plot.bar(color='red')
corr=patients.corr() 
from sklearn.model_selection import train_test_split #we will be using the train-test method for our model
patients.columns #finding out necessary variables
X=patients[['Age', 'Gender', 'Total_Bilirubin', 'Direct_Bilirubin',        #Setting our variables for X and y
       'Alkaline_Phosphotase', 'Alamine_Aminotransferase',
       'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',
       'Albumin_and_Globulin_Ratio']]
y=patients['Dataset']
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123) #fitting our variables for the train-test-split method

from sklearn.linear_model import LogisticRegression      #Importing Logistic Regression as our machine learning model
from sklearn.metrics import accuracy_score, confusion_matrix  #importing accuracy and confuson matrix for validation testing
logmodel = LogisticRegression()
log5 = logmodel.fit(X_train,y_train)    #fitting our model

predictions = log5.predict(X_test) #making our model predict results
print(predictions)
from sklearn import metrics #printing out the predictions on to the confsuion matrix
cnf_matrix = metrics.confusion_matrix(y_test,predictions)
cnf_matrixclass_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)   #Making our confusion matrix on a chart, results show that its a 65:35 ratio of Disease:No Disease
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
from sklearn import metrics #printing out the predictions on to the confsuion matrix
cnf_matrix = metrics.confusion_matrix(y_test,predictions)
cnf_matrixclass_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)   #Making our confusion matrix on a chart, results show that its a 65:35 ratio of Disease:No Disease
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
from sklearn.model_selection import KFold, cross_val_score   #Getting our accuracy scores, it was 58.67% for the first test; we then improved indicators of our model and hence got 71.5%!
kfold = KFold(n_splits=5,random_state=42)
logmodel = LogisticRegression(C=1, penalty='l1')
results = cross_val_score(logmodel, X_train,y_train,cv = kfold)
print(results)
print("Accuracy:",results.mean()*100)

